import torch
import torch.nn as nn
import torch.optim as optim
from torchtext.vocab import build_vocab_from_iterator
import time
import re
import pandas as pd
from pathlib import Path
from collections import defaultdict
from typing import List, Tuple

# ç¡¬ä»¶åŠ é€Ÿè‡ªåŠ¨åˆå§‹åŒ–ï¼ˆä¸å˜ï¼‰
def init_hardware_acceleration():
    if torch.cuda.is_available():
        device = torch.device("cuda")
        device_name = torch.cuda.get_device_name(0)
        acceleration_type = "NVIDIA GPU/CUDA"
    elif torch.backends.mps.is_available():
        device = torch.device("mps")
        device_name = "Apple Metal GPU"
        acceleration_type = "Metal GPU"
    elif hasattr(torch.backends, 'openvino') and torch.backends.openvino.is_available():
        device = torch.device("openvino")
        device_name = "Intel iGPU"
        acceleration_type = "Intel OpenVINO"
    else:
        device = torch.device("cpu")
        device_name = "CPU"
        acceleration_type = "CPU SIMD"
    
    print(f"======================================")
    print(f"âœ… ç¡¬ä»¶åŠ é€Ÿå·²å¯åŠ¨ï¼ï¼ˆæ”¯æŒåŒç²¾åº¦è‡ªåŠ¨å­¦ä¹ ï¼‰")
    print(f"åŠ é€Ÿç±»å‹ï¼š{acceleration_type}")
    print(f"è®¾å¤‡åç§°ï¼š{device_name}")
    print(f"å½“å‰è®¾å¤‡ï¼š{device} | æ•°æ®ç±»å‹ï¼štorch.float64ï¼ˆåŒç²¾ï¼‰")
    print(f"======================================")
    return device

# æ–‡æœ¬é¢„å¤„ç†ï¼ˆä¸å˜ï¼‰
def preprocess_text(text):
    text = str(text).lower().strip()
    text = re.sub(r'[^\w\s]', '', text)
    return text.split()

# è‡ªåŠ¨æ„å»ºè¯æ±‡è¡¨ï¼ˆä¸å˜ï¼‰
def build_vocab(texts):
    def yield_tokens(texts):
        for text in texts:
            yield preprocess_text(text)
    vocab = build_vocab_from_iterator(yield_tokens(texts), specials=["<pad>", "<unk>"])
    vocab.set_default_index(vocab["<unk>"])
    return vocab

# è‡ªåŠ¨å­¦ä¹ +è¯­ä¹‰ç†è§£æ¨¡å‹ï¼ˆä¸å˜ï¼‰
class AutoUnderstandingModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, num_classes=2, dtype=torch.float64):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim, dtype=dtype)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True, dtype=dtype)
        self.fc = nn.Linear(hidden_dim * 2, num_classes, dtype=dtype)
        self.dtype = dtype

    def forward(self, x):
        embed = self.embedding(x)
        lstm_out, _ = self.lstm(embed)
        cls_feat = lstm_out[:, -1, :]
        logits = self.fc(cls_feat)
        return logits

# æ•°æ®åŠ è½½ï¼ˆä¸å˜ï¼‰
def load_data(texts, labels, vocab, max_seq_len=32, device="cpu"):
    def encode_text(text):
        tokens = preprocess_text(text)
        ids = vocab(tokens)[:max_seq_len]
        ids += [vocab["<pad>"]] * (max_seq_len - len(ids))
        return torch.tensor(ids, dtype=torch.long, device=device)
    
    X = torch.stack([encode_text(text) for text in texts])
    y = torch.tensor(labels, dtype=torch.long, device=device)
    return X, y

# å•ä¸ªæ–‡ä»¶åŠ è½½ï¼ˆåŸºç¡€å‡½æ•°ï¼Œä¸å˜ï¼‰
def load_single_file(file_path: str, text_col="text", label_col="label") -> Tuple[List[str], List[int]]:
    file_path = Path(file_path)
    if not file_path.exists():
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨ï¼š{file_path}")
    
    if file_path.suffix == ".csv":
        df = pd.read_csv(file_path)
    elif file_path.suffix == ".tsv":
        df = pd.read_csv(file_path, sep="\t")
    elif file_path.suffix == ".txt":
        df = pd.read_csv(file_path, sep="\t", names=[text_col, label_col])
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼š{file_path.suffix}ï¼Œä»…æ”¯æŒ CSV/TSV/TXT")
    
    if text_col not in df.columns or label_col not in df.columns:
        raise ValueError(f"æ–‡ä»¶éœ€åŒ…å«åˆ—ï¼š{text_col}ï¼ˆæ–‡æœ¬ï¼‰å’Œ {label_col}ï¼ˆæ ‡ç­¾ï¼‰")
    
    texts = df[text_col].dropna().unique().tolist()
    labels = df[df[text_col].isin(texts)][label_col].astype(int).tolist()
    return texts, labels

# æ–°å¢ï¼šæ‰¹é‡åŠ è½½å¤šä¸ªæ–‡ä»¶å¹¶åˆå¹¶
def load_multiple_files(file_list: List[str], text_col="text", label_col="label") -> Tuple[List[str], List[int]]:
    """
    æ‰¹é‡åŠ è½½å¤šä¸ªæ•°æ®æ–‡ä»¶ï¼Œè‡ªåŠ¨åˆå¹¶æ–‡æœ¬å’Œæ ‡ç­¾
    :param file_list: æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆå¦‚ ["data1.csv", "data2.tsv", "data3.txt"]ï¼‰
    :return: åˆå¹¶åçš„ texts, labels
    """
    all_texts = []
    all_labels = []
    
    for file in file_list:
        print(f"\nğŸ“„ æ­£åœ¨åŠ è½½æ–‡ä»¶ï¼š{file}")
        try:
            texts, labels = load_single_file(file, text_col, label_col)
            all_texts.extend(texts)
            all_labels.extend(labels)
            print(f"âœ… åŠ è½½æˆåŠŸï¼š{len(texts)} æ¡æ•°æ®")
        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥ï¼š{e}ï¼Œè·³è¿‡è¯¥æ–‡ä»¶")
    
    # å»é‡ï¼ˆé¿å…å¤šä¸ªæ–‡ä»¶ä¸­çš„é‡å¤æ–‡æœ¬ï¼‰
    unique_texts = []
    unique_labels = []
    text_set = set()
    for text, label in zip(all_texts, all_labels):
        if text not in text_set:
            text_set.add(text)
            unique_texts.append(text)
            unique_labels.append(label)
    
    print(f"\nğŸ“Š æ‰¹é‡åŠ è½½å®Œæˆï¼šå…± {len(unique_texts)} æ¡uniqueæ•°æ®ï¼Œ{len(set(unique_labels))} ä¸ªç±»åˆ«")
    return unique_texts, unique_labels

# è‡ªåŠ¨å­¦ä¹ æµç¨‹ï¼ˆä¿®æ”¹ä¸ºæ”¯æŒå¤šæ–‡ä»¶ï¼‰
def auto_learn_and_understand(
    device,
    file_list: List[str] = ["train1.csv", "train2.tsv", "train3.txt"],  # é»˜è®¤å¤šæ–‡ä»¶åˆ—è¡¨
    text_col="text",
    label_col="label"
):
    # 1. æ‰¹é‡åŠ è½½å¤šä¸ªæ–‡ä»¶
    print(f"ğŸ“‚ å¯åŠ¨æ‰¹é‡æ•°æ®åŠ è½½ï¼šå…± {len(file_list)} ä¸ªæ–‡ä»¶")
    try:
        train_texts, train_labels = load_multiple_files(file_list, text_col, label_col)
        if not train_texts:
            print(f"âŒ æ— æœ‰æ•ˆæ•°æ®åŠ è½½ï¼Œç¨‹åºç»ˆæ­¢")
            return
    except Exception as e:
        print(f"âŒ æ‰¹é‡åŠ è½½å¼‚å¸¸ï¼š{e}")
        return
    
    # 2. è‡ªåŠ¨æ„å»ºè¯æ±‡è¡¨
    print(f"\nğŸ“š å¯åŠ¨è‡ªåŠ¨ç‰¹å¾å­¦ä¹ ï¼šæ„å»ºæ–‡æœ¬è¯æ±‡è¡¨")
    vocab = build_vocab(train_texts)
    vocab_size = len(vocab)
    print(f"âœ… è¯æ±‡è¡¨æ„å»ºå®Œæˆï¼ˆè§„æ¨¡ï¼š{vocab_size} ä¸ªè¯ï¼‰")
    
    # 3. åŠ è½½æ•°æ®ï¼ˆåŒç²¾åº¦é€‚é…ï¼‰
    X_train, y_train = load_data(train_texts, train_labels, vocab, device=device)
    
    # 4. åˆå§‹åŒ–æ¨¡å‹ï¼ˆåŠ¨æ€é€‚é…ç±»åˆ«æ•°ï¼‰
    num_classes = len(set(train_labels))
    model = AutoUnderstandingModel(vocab_size, num_classes=num_classes, dtype=torch.float64).to(device)
    criterion = nn.CrossEntropyLoss(dtype=torch.float64)
    optimizer = optim.Adam(model.parameters(), lr=1e-3)
    
    # 5. è‡ªåŠ¨è®­ç»ƒ
    print(f"\nâš¡ å¯åŠ¨åŒç²¾åº¦è‡ªåŠ¨è®­ç»ƒï¼ˆç¡¬ä»¶åŠ é€Ÿï¼Œ{num_classes} åˆ†ç±»ï¼Œ{len(train_texts)} æ¡æ•°æ®ï¼‰")
    epochs = 10
    model.train()
    start_time = time.perf_counter()
    
    for epoch in range(epochs):
        optimizer.zero_grad()
        outputs = model(X_train)
        loss = criterion(outputs, y_train)
        loss.backward()
        optimizer.step()
        
        if (epoch + 1) % 5 == 0:
            elapsed = time.perf_counter() - start_time
            print(f"Epoch [{epoch+1}/{epochs}] | æŸå¤±ï¼š{loss.item():.6f}ï¼ˆåŒç²¾ï¼‰ | è€—æ—¶ï¼š{elapsed:.6f}ç§’")
    
    # 6. ç¤ºä¾‹æµ‹è¯•ï¼ˆå¯æ›¿æ¢ä¸ºæ–‡ä»¶æµ‹è¯•æ•°æ®ï¼‰
    test_texts = [
        "å¤§è¯­è¨€æ¨¡å‹èƒ½ç†è§£äººç±»è¯­è¨€", "å‡æœŸå»æµ·è¾¹åº¦å‡",
        "æ·±åº¦å­¦ä¹ ä¼˜åŒ–æ¨¡å‹æ€§èƒ½", "ä»Šå¤©çš„å’–å•¡å¾ˆç¾å‘³",
        "è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯æŒç»­è¿›æ­¥", "å‘¨æœ«å’Œæœ‹å‹å»éœ²è¥"
    ]
    X_test, _ = load_data(test_texts, [0]*len(test_texts), vocab, device=device)
    
    # 7. è‡ªåŠ¨ç†è§£æ¨ç†
    model.eval()
    with torch.no_grad():
        infer_start = time.perf_counter()
        outputs = model(X_test)
        preds = torch.argmax(outputs, dim=1)
        infer_time = time.perf_counter() - infer_start
    
    # 8. è¾“å‡ºç»“æœ
    label_map = {i: f"ç±»åˆ«{i}" for i in sorted(set(train_labels))}
    print(f"\nğŸ“ˆ è‡ªåŠ¨ç†è§£ç»“æœï¼ˆåŒç²¾åº¦æ¨ç†è€—æ—¶ï¼š{infer_time:.6f}